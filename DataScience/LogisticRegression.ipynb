{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The dataset that was used for this lab is [here](https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Description\n",
    "\n",
    "#### a. Describe the Dataset\n",
    "This data set contains a list of blood doners that gave blood to a specific blood transfusion center.     \n",
    "#### b. Feature Representation\n",
    "The features are: \"Recency\" which represent how recent the donor gave blood, \"Frequency\" represents the total amount of times given, \"Monetary\" which is the total amount of blood given in c.c.s, \"Time\" represents the total amount of months since the first donation, and \"Donated\" is a binary value that represents whether or not a specific blood donor gave blood in March 2007.     \n",
    "#### c. Target Variable\n",
    "The target variable in this data set is the \"Donated\" value in each of the data points. We use the other features in order to predict this value.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score, accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Monetary</th>\n",
       "      <th>Time</th>\n",
       "      <th>Donated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency  Frequency  Monetary  Time  Donated\n",
       "0        2         50     12500    98        1\n",
       "1        0         13      3250    28        1\n",
       "2        1         16      4000    35        1\n",
       "3        2         20      5000    45        1\n",
       "4        1         24      6000    77        0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\")\n",
    "\n",
    "#dataset[\"Bias\"] = 1\n",
    "dataset.columns = [\"Recency\", \"Frequency\", \"Monetary\", \"Time\", \"Donated\"]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop([\"Donated\"], axis = 1)\n",
    "Y = dataset.Donated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((523, 4), (225, 4), (523,), (225,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "logReg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.78\n",
      "Test accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "logReg.predict(X_test)\n",
    "\n",
    "acc = logReg.score(X_train, Y_train)\n",
    "\n",
    "print('Train accuracy: {:.2f}'.format(acc))\n",
    "\n",
    "print('Test accuracy: {:.2f}'.format(logReg.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.93      0.81       162\n",
      "           1       0.21      0.05      0.08        63\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       225\n",
      "   macro avg       0.46      0.49      0.44       225\n",
      "weighted avg       0.58      0.68      0.60       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86722379, 0.13277621],\n",
       "       [0.8161895 , 0.1838105 ],\n",
       "       [0.96150993, 0.03849007],\n",
       "       ...,\n",
       "       [0.96650458, 0.03349542],\n",
       "       [0.87182875, 0.12817125],\n",
       "       [0.64575612, 0.35424388]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15486996]\n",
      "[[-6.28934336e-02  9.11092719e-07  2.27773180e-04 -1.07525510e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(logReg.intercept_) \n",
    "print(logReg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_0$ = -0.26037524      \n",
    "$\\theta_1$ = -0.06095942117   \n",
    "$\\theta_2$ = -0.000000980667403   \n",
    "$\\theta_3$ = -0.000245166851    \n",
    "$\\theta_4$ = -0.0073296504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different thetas represent the different weights that each of the variables, or features, carries with the logistic regresssion calculation.     \n",
    "\n",
    "$\\theta_0$ represents the weight of the bias of the logistic function. It also shows as the y-intercept when graphing the logistic function. The $\\theta_1$ represents the weight that the \"Recency\" feature carries when calculating the regression. Since $\\theta_1$ is the highest number, the \"Recency\" feature carries the most amount of weight out of the other features in the calculation.      \n",
    "      \n",
    "$\\theta_2$ represents the weight that the \"Frequency\" feature of each data point carries within the logistic calculation. When comparing to the other $\\theta$s,  $\\theta_2$ has the lowest amount of weight; therefore, when used in the calculations, it still matters, but not nearly as much as the other features in the data point. The $\\theta_3$ weight represents the amount of weight that the \"Monetary\" tab carries within the calculation of the logistic regression of the data set. The $\\theta_4$ weight represents the weight that the \"Time\" feature within each data point carries when that data point is being used in the calculation of logistic regression of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statement of Collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Whom you worked with\n",
    "I mainly did this by myself, but I did get help from Kolby, Matt and Tucker.    \n",
    "      \n",
    "#### b. Resources Used\n",
    "The only resources I used were trying to figure out Logistic Regressison using SkLearn and other websites similar to that.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Logistic Regression by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Article\n",
    "     \n",
    "EssentiLly the article begins with the idea of the author wanting to get more in depth with machine learning, and as he got more in depth with it, he learned he had to become more familiar with statistics and statistical terms. He said that he wanted to write this blog post to explain to the average person what some of the specific terms mean. \n",
    "\n",
    "The author then defines a few terms with emphasis on Entropy. He goes on to describe entropy as being something used for probability distributions and that it \"measures the uncertainty inherent in their probability distribution.\" From there he begins to go into the idea of response variables. At the start of this section, he describes the concept of a model taking in a specific input and returning a desired output. The author describes the idea of a maximum entropy distrubtions basically being a distrbution that obeys specific constraints. After that, the author gives a brief description/equation of other distributions such as Gaussian Distribution, Binomial Distirbution, and Multinomial Distributions. \n",
    "\n",
    "From the response variables, he begins to go into the idea of functional form, starting with exponential family distribtuions. A large portion of this section outlines the math behind the distributions above and how they work. Generalized linear models is up next, and here is goes into the ideas of regression, specifically Linear Regression, Logistic Regression and Softmax Regression with some emphasis on why the functions associated with them are associated with them. He then goes on to the loss function and he describes it as being a way to compute how good specific parameters are. The author then uses the generalized linear models to show this using maximum likelihood estimation. The author then uses the same models for maximum a posteriori estimation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
